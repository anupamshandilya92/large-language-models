{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we evaluate the fine tuned Gemma 2b model. Evaluation of machine translation task is a tough task. Translation is an open ended task, a sentence can have multiple correct translations. To ensure that the translations capture the right context, cultural nuances, idiomatic expressions, evaluations need to be evaluated manually by humans. Various scores available miss these aspects of translations. Human evaluations are expensive, and for our task we still rely on commonly used metrics used for translation task. \n",
    "\n",
    "We make use of BiLingual Evaluation Understudy score (BLEU score) proposed by Papineni, K., et al. fo evaluting our fine tuned model. This metric helps evaluating the quality of machine-translated text by comparing it to one or more reference translations. The score ranges from 0 to 1, with 1 indicating a perfect match between the machine translation and the reference translation(s).\n",
    "\n",
    "To see how much our finetuning has improved the model, we take a random sample of 1000 examples from the CFILT dataset that was not used in fine tuning the Gemma model. We compare the BLEU score for baseline model with that of fine tuned model on this sample. Further, we also compare how our fine tuned model performs against Nemotron-4-Mini-Hindi-4B-Instruct model released by Nvidia on this test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T21:55:58.366680Z",
     "iopub.status.busy": "2025-01-15T21:55:58.366306Z",
     "iopub.status.idle": "2025-01-15T21:56:38.331589Z",
     "shell.execute_reply": "2025-01-15T21:56:38.330691Z",
     "shell.execute_reply.started": "2025-01-15T21:55:58.366646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp datasets\n",
    "!pip install -q -U keras\n",
    "\n",
    "import os\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TrainingArguments, pipeline, AutoModelForCausalLM\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:56:45.253482Z",
     "iopub.status.busy": "2025-01-15T21:56:45.252864Z",
     "iopub.status.idle": "2025-01-15T21:56:45.258246Z",
     "shell.execute_reply": "2025-01-15T21:56:45.257255Z",
     "shell.execute_reply.started": "2025-01-15T21:56:45.253446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configs \n",
    "# Set the backbend before importing Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "model_id = \"gemma2_instruct_2b_en\"\n",
    "token_limit = 256\n",
    "\n",
    "# Run at half precision.\n",
    "#keras.config.set_floatx(\"bfloat16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a util function we use to calculate BLEU score, it uses the sentence_bleu function available in NLTK package. For calculating BLEU score, we build n-grams of translated and reference text, and calculate the precision of translated text for each n-gram (1-gram, 2-gram, 3-gram and so on). To prevent the machine translation from getting undue credit for repeating words excessively, we limit the count of n-grams in the machine-generated translation to the maximum count found in any reference translation. This is known as clipping. The final score is calculated by taking geometric mean of the precisions at each n-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:56:46.640662Z",
     "iopub.status.busy": "2025-01-15T21:56:46.640067Z",
     "iopub.status.idle": "2025-01-15T21:56:46.645405Z",
     "shell.execute_reply": "2025-01-15T21:56:46.644477Z",
     "shell.execute_reply.started": "2025-01-15T21:56:46.640626Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "def clean_model_output(txt):\n",
    "    \"\"\"Removes the control tokens from model output \"\"\"\n",
    "    txt = txt[txt.find('model') + 6 : -14]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:56:51.694980Z",
     "iopub.status.busy": "2025-01-15T21:56:51.694633Z",
     "iopub.status.idle": "2025-01-15T21:56:51.699452Z",
     "shell.execute_reply": "2025-01-15T21:56:51.698600Z",
     "shell.execute_reply.started": "2025-01-15T21:56:51.694952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helpers : https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb\n",
    "def convert_message_to_prompt(message: str, model_prefix: str = \"\") -> str:\n",
    "    \"\"\"Converts a message to a prompt for a large language model.\n",
    "\n",
    "    Args:\n",
    "        message: The message to convert (str).\n",
    "        model_prefix: An optional prefix to prepend to the model response (str).\n",
    "\n",
    "    Returns:\n",
    "        A string containing the prompt for the large language model (str).\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        f\"<start_of_turn>user\\n Translate the sentence into hindi and only return the translation. Text :  {message}<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n{model_prefix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:56:52.655972Z",
     "iopub.status.busy": "2025-01-15T21:56:52.655155Z",
     "iopub.status.idle": "2025-01-15T21:57:00.961829Z",
     "shell.execute_reply": "2025-01-15T21:57:00.960883Z",
     "shell.execute_reply.started": "2025-01-15T21:56:52.655937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1659083, 1)\n",
      "(520, 1)\n",
      "(2507, 1)\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "\n",
    "# load the parquet files from huggingface\n",
    "train = pd.read_parquet(\n",
    "    \"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"train\"]\n",
    ")\n",
    "val = pd.read_parquet(\n",
    "    \"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"validation\"]\n",
    ")\n",
    "test = pd.read_parquet(\n",
    "    \"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"test\"]\n",
    ")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model details : Gemma 2\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "Key Architecture Highlights:\n",
    "\n",
    "1. GeGelu activation\n",
    "2. RoPe positional embeddings\n",
    "3. Multi Query Attention\n",
    "4. RMSNorm\n",
    "\n",
    "Training Highlights:\n",
    "1. Pretraining with 2 trillion tokens. \n",
    "2. sentence piece token/byte level tokenization for unknown tokens\n",
    "3. Teacher student training with 27b model\n",
    "4. dictionary size\n",
    "5. Instruction fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T20:34:01.727737Z",
     "iopub.status.busy": "2025-01-15T20:34:01.727255Z",
     "iopub.status.idle": "2025-01-15T20:34:57.441289Z",
     "shell.execute_reply": "2025-01-15T20:34:57.440571Z",
     "shell.execute_reply.started": "2025-01-15T20:34:01.727700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "with tf.device('/GPU:0'):\n",
    "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
    "    gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T20:33:14.797655Z",
     "iopub.status.busy": "2025-01-15T20:33:14.797379Z",
     "iopub.status.idle": "2025-01-15T20:33:14.850231Z",
     "shell.execute_reply": "2025-01-15T20:33:14.849132Z",
     "shell.execute_reply.started": "2025-01-15T20:33:14.797630Z"
    }
   },
   "outputs": [],
   "source": [
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    " \n",
    "def text_gen(prompt, debug = False):\n",
    "    input = convert_message_to_prompt(prompt)\n",
    "    if debug:\n",
    "        print(f'User input: {input}')\n",
    "    output = gemma_lm.generate(input, max_length=token_limit)\n",
    "    if debug:\n",
    "        print(\"\\nGemma output:\")\n",
    "        print(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating base model - blue score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:57:33.780061Z",
     "iopub.status.busy": "2025-01-15T21:57:33.779688Z",
     "iopub.status.idle": "2025-01-15T21:57:33.809332Z",
     "shell.execute_reply": "2025-01-15T21:57:33.808369Z",
     "shell.execute_reply.started": "2025-01-15T21:57:33.780029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>{'en': 'Due to financial constraints Dhirubhai...</td>\n",
       "      <td>Due to financial constraints Dhirubhai had to ...</td>\n",
       "      <td>आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'en': 'Or they can choose not to have a devic...</td>\n",
       "      <td>Or they can choose not to have a device at all...</td>\n",
       "      <td>या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>{'en': 'However, following the recent murder o...</td>\n",
       "      <td>However, following the recent murder of Austra...</td>\n",
       "      <td>वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>{'en': 'Delta and JetBlue were among the airli...</td>\n",
       "      <td>Delta and JetBlue were among the airliners who...</td>\n",
       "      <td>डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>{'en': 'It was simply nothing I could have ima...</td>\n",
       "      <td>It was simply nothing I could have imagined.</td>\n",
       "      <td>सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            translation  \\\n",
       "2121  {'en': 'Due to financial constraints Dhirubhai...   \n",
       "56    {'en': 'Or they can choose not to have a devic...   \n",
       "2479  {'en': 'However, following the recent murder o...   \n",
       "1292  {'en': 'Delta and JetBlue were among the airli...   \n",
       "1599  {'en': 'It was simply nothing I could have ima...   \n",
       "\n",
       "                                                english  \\\n",
       "2121  Due to financial constraints Dhirubhai had to ...   \n",
       "56    Or they can choose not to have a device at all...   \n",
       "2479  However, following the recent murder of Austra...   \n",
       "1292  Delta and JetBlue were among the airliners who...   \n",
       "1599       It was simply nothing I could have imagined.   \n",
       "\n",
       "                                                  hindi  \n",
       "2121  आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...  \n",
       "56    या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...  \n",
       "2479  वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...  \n",
       "1292  डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...  \n",
       "1599  सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = test.sample(1000, random_state=42)\n",
    "print(test_sample.shape)\n",
    "test_sample[\"english\"] = test_sample.translation.apply(lambda x: x.get(\"en\"))\n",
    "test_sample[\"hindi\"] = test_sample.translation.apply(lambda x: x.get(\"hi\"))\n",
    "\n",
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T20:34:57.442986Z",
     "iopub.status.busy": "2025-01-15T20:34:57.442720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736973306.900328      23 service.cc:145] XLA service 0x589d722899e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736973306.900392      23 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1736973319.521795      23 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "tick()\n",
    "test_sample[\"predictions_woft\"] = test_sample.english.apply(\n",
    "    lambda text: text_gen(f\"{text}\")\n",
    ")\n",
    "tock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:01:08.905417Z",
     "iopub.status.busy": "2025-01-15T21:01:08.905172Z",
     "iopub.status.idle": "2025-01-15T21:01:08.914942Z",
     "shell.execute_reply": "2025-01-15T21:01:08.914121Z",
     "shell.execute_reply.started": "2025-01-15T21:01:08.905393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "      <th>predictions_woft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>{'en': 'Due to financial constraints Dhirubhai...</td>\n",
       "      <td>Due to financial constraints Dhirubhai had to ...</td>\n",
       "      <td>आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'en': 'Or they can choose not to have a devic...</td>\n",
       "      <td>Or they can choose not to have a device at all...</td>\n",
       "      <td>या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>{'en': 'However, following the recent murder o...</td>\n",
       "      <td>However, following the recent murder of Austra...</td>\n",
       "      <td>वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>{'en': 'Delta and JetBlue were among the airli...</td>\n",
       "      <td>Delta and JetBlue were among the airliners who...</td>\n",
       "      <td>डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>{'en': 'It was simply nothing I could have ima...</td>\n",
       "      <td>It was simply nothing I could have imagined.</td>\n",
       "      <td>सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            translation  \\\n",
       "2121  {'en': 'Due to financial constraints Dhirubhai...   \n",
       "56    {'en': 'Or they can choose not to have a devic...   \n",
       "2479  {'en': 'However, following the recent murder o...   \n",
       "1292  {'en': 'Delta and JetBlue were among the airli...   \n",
       "1599  {'en': 'It was simply nothing I could have ima...   \n",
       "\n",
       "                                                english  \\\n",
       "2121  Due to financial constraints Dhirubhai had to ...   \n",
       "56    Or they can choose not to have a device at all...   \n",
       "2479  However, following the recent murder of Austra...   \n",
       "1292  Delta and JetBlue were among the airliners who...   \n",
       "1599       It was simply nothing I could have imagined.   \n",
       "\n",
       "                                                  hindi  \\\n",
       "2121  आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...   \n",
       "56    या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...   \n",
       "2479  वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...   \n",
       "1292  डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...   \n",
       "1599  सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...   \n",
       "\n",
       "                                       predictions_woft  \n",
       "2121  <start_of_turn>user\\n Translate the sentence i...  \n",
       "56    <start_of_turn>user\\n Translate the sentence i...  \n",
       "2479  <start_of_turn>user\\n Translate the sentence i...  \n",
       "1292  <start_of_turn>user\\n Translate the sentence i...  \n",
       "1599  <start_of_turn>user\\n Translate the sentence i...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:06:30.828298Z",
     "iopub.status.busy": "2025-01-15T21:06:30.827952Z",
     "iopub.status.idle": "2025-01-15T21:06:30.834710Z",
     "shell.execute_reply": "2025-01-15T21:06:30.833767Z",
     "shell.execute_reply.started": "2025-01-15T21:06:30.828266Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sample[\"predictions_woft_clean\"] = test_sample.predictions_woft.apply(\n",
    "    lambda text: clean_model_output(text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:06:33.491579Z",
     "iopub.status.busy": "2025-01-15T21:06:33.490779Z",
     "iopub.status.idle": "2025-01-15T21:06:34.004240Z",
     "shell.execute_reply": "2025-01-15T21:06:34.003362Z",
     "shell.execute_reply.started": "2025-01-15T21:06:33.491545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.3\n"
     ]
    }
   ],
   "source": [
    "test_sample[\"BLEU_Score\"] = test_sample[\n",
    "    [\"hindi\", \"predictions_woft_clean\"]\n",
    "].apply(\n",
    "    lambda inputs: calculate_bleu_score(inputs[0], inputs[1]), axis=1\n",
    ")\n",
    "\n",
    "print(f\"Average BLEU Score: {test_sample['BLEU_Score'].mean().round(2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A BLEU score of 0.3 (or 30) means that the machine translation has a moderate level of overlap with the reference translation(s). This score indicates that approximately one-third of the n-grams (contiguous sequences of words) in the machine-translated text match those in the reference translation(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:29:03.804362Z",
     "iopub.status.busy": "2025-01-15T21:29:03.804018Z",
     "iopub.status.idle": "2025-01-15T21:29:03.812461Z",
     "shell.execute_reply": "2025-01-15T21:29:03.811629Z",
     "shell.execute_reply.started": "2025-01-15T21:29:03.804330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original english sentence :\n",
      "Due to financial constraints Dhirubhai had to drop out after high school.\n",
      "Hindi ground truth sentence : \n",
      "आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद ही पढ़ाई छोड़ना पड़ गई।\n",
      "Hindi translation :\n",
      "धन कमी के कारण दिरुभाई ने हाई स्कूल छोड़ दिया। \n",
      "BLEU score for this sentence :\n",
      "0.25880882365505126\n"
     ]
    }
   ],
   "source": [
    "sample_trn = test_sample[[\"hindi\", \"predictions_woft_clean\", \"english\", \"BLEU_Score\"]].head(1).to_dict()\n",
    "print(f\"Original english sentence :\\n{sample_trn['english'][2121]}\")\n",
    "print(f\"Hindi ground truth sentence : \\n{sample_trn['hindi'][2121]}\")\n",
    "print(f\"Hindi translation :\\n{sample_trn['predictions_woft_clean'][2121]}\")\n",
    "print(f\"BLEU score for this sentence :\\n{sample_trn['BLEU_Score'][2121]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:09:54.008544Z",
     "iopub.status.busy": "2025-01-15T21:09:54.008183Z",
     "iopub.status.idle": "2025-01-15T21:09:54.014316Z",
     "shell.execute_reply": "2025-01-15T21:09:54.013321Z",
     "shell.execute_reply.started": "2025-01-15T21:09:54.008495Z"
    }
   },
   "source": [
    "BLEU of 0.25 here means that roughly one fourth of tokens have overlap here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuned Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:14:47.855572Z",
     "iopub.status.busy": "2025-01-15T22:14:47.854831Z",
     "iopub.status.idle": "2025-01-15T22:15:41.533693Z",
     "shell.execute_reply": "2025-01-15T22:15:41.532701Z",
     "shell.execute_reply.started": "2025-01-15T22:14:47.855539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n",
    "#     # # Use the same LoRA rank that you trained\n",
    "    gemma_lm.backbone.enable_lora(rank=20)\n",
    "    \n",
    "    # Load pre-trained LoRA weights\n",
    "    gemma_lm.backbone.load_lora_weights(f\"/kaggle/input/gemma2b-fine-tuned/llm_mark1_20_epoch20.lora (1).h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:15:41.536299Z",
     "iopub.status.busy": "2025-01-15T22:15:41.535615Z",
     "iopub.status.idle": "2025-01-15T22:15:41.573178Z",
     "shell.execute_reply": "2025-01-15T22:15:41.572487Z",
     "shell.execute_reply.started": "2025-01-15T22:15:41.536254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,628,985,088</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,628,985,088\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,628,985,088</span> (9.79 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,628,985,088\u001b[0m (9.79 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,643,200</span> (55.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,643,200\u001b[0m (55.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.compile()#sampler=\"top_k\"\n",
    "\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:15:41.574318Z",
     "iopub.status.busy": "2025-01-15T22:15:41.574093Z",
     "iopub.status.idle": "2025-01-15T22:15:41.578974Z",
     "shell.execute_reply": "2025-01-15T22:15:41.578105Z",
     "shell.execute_reply.started": "2025-01-15T22:15:41.574294Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_gen(prompt, debug = False):\n",
    "    input = convert_message_to_prompt(prompt)\n",
    "    if debug:\n",
    "        print(f'User input: {input}')\n",
    "    output = gemma_lm.generate(input, max_length=token_limit)\n",
    "    if debug:\n",
    "        print(\"\\nGemma output:\")\n",
    "        print(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test Set - Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:15:41.581089Z",
     "iopub.status.busy": "2025-01-15T22:15:41.580362Z",
     "iopub.status.idle": "2025-01-15T22:44:08.655920Z",
     "shell.execute_reply": "2025-01-15T22:44:08.654391Z",
     "shell.execute_reply.started": "2025-01-15T22:15:41.581061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736979351.890667      23 service.cc:145] XLA service 0x5bfdbe6f5020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736979351.891216      23 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1736979366.092412      23 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sample[\"predictions_ft\"] = test_sample.english.apply(\n",
    "    lambda text: text_gen(f\"{text}\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:44:08.657658Z",
     "iopub.status.busy": "2025-01-15T22:44:08.657255Z",
     "iopub.status.idle": "2025-01-15T22:44:08.678720Z",
     "shell.execute_reply": "2025-01-15T22:44:08.677630Z",
     "shell.execute_reply.started": "2025-01-15T22:44:08.657617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "      <th>predictions_ft</th>\n",
       "      <th>predictions_ft_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>{'en': 'Due to financial constraints Dhirubhai...</td>\n",
       "      <td>Due to financial constraints Dhirubhai had to ...</td>\n",
       "      <td>आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>दुर्घटना के बैंक को उ उठाना पड़े।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'en': 'Or they can choose not to have a devic...</td>\n",
       "      <td>Or they can choose not to have a device at all...</td>\n",
       "      <td>या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>उन्होंने अपने राज्य के द्‌ swagger खाते को चुन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>{'en': 'However, following the recent murder o...</td>\n",
       "      <td>However, following the recent murder of Austra...</td>\n",
       "      <td>वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>हालांकि, 2008 में कोचीन के आगंतुक़ाठुइ ने शाहि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>{'en': 'Delta and JetBlue were among the airli...</td>\n",
       "      <td>Delta and JetBlue were among the airliners who...</td>\n",
       "      <td>डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>दूसरी दुनिया पर उड़ानों पर उड़ान की योजनाएं आर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>{'en': 'It was simply nothing I could have ima...</td>\n",
       "      <td>It was simply nothing I could have imagined.</td>\n",
       "      <td>सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>यह सिर्फ एक युद्ध नहीं था,Décès मेरें,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            translation  \\\n",
       "2121  {'en': 'Due to financial constraints Dhirubhai...   \n",
       "56    {'en': 'Or they can choose not to have a devic...   \n",
       "2479  {'en': 'However, following the recent murder o...   \n",
       "1292  {'en': 'Delta and JetBlue were among the airli...   \n",
       "1599  {'en': 'It was simply nothing I could have ima...   \n",
       "\n",
       "                                                english  \\\n",
       "2121  Due to financial constraints Dhirubhai had to ...   \n",
       "56    Or they can choose not to have a device at all...   \n",
       "2479  However, following the recent murder of Austra...   \n",
       "1292  Delta and JetBlue were among the airliners who...   \n",
       "1599       It was simply nothing I could have imagined.   \n",
       "\n",
       "                                                  hindi  \\\n",
       "2121  आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...   \n",
       "56    या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...   \n",
       "2479  वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...   \n",
       "1292  डेल्टा और जेटब्लू उन एयरलाइन्स में से हैं, जिन...   \n",
       "1599  सरल तौर कहें तो में इसमें से किसी भी कल्पना भी...   \n",
       "\n",
       "                                         predictions_ft  \\\n",
       "2121  <start_of_turn>user\\n Translate the sentence i...   \n",
       "56    <start_of_turn>user\\n Translate the sentence i...   \n",
       "2479  <start_of_turn>user\\n Translate the sentence i...   \n",
       "1292  <start_of_turn>user\\n Translate the sentence i...   \n",
       "1599  <start_of_turn>user\\n Translate the sentence i...   \n",
       "\n",
       "                                   predictions_ft_clean  \n",
       "2121                  दुर्घटना के बैंक को उ उठाना पड़े।  \n",
       "56    उन्होंने अपने राज्य के द्‌ swagger खाते को चुन...  \n",
       "2479  हालांकि, 2008 में कोचीन के आगंतुक़ाठुइ ने शाहि...  \n",
       "1292  दूसरी दुनिया पर उड़ानों पर उड़ान की योजनाएं आर...  \n",
       "1599             यह सिर्फ एक युद्ध नहीं था,Décès मेरें,  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample[\"predictions_ft_clean\"] = test_sample.predictions_ft.apply(\n",
    "    lambda text: clean_model_output(text)\n",
    ")\n",
    "\n",
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:44:08.680350Z",
     "iopub.status.busy": "2025-01-15T22:44:08.679994Z",
     "iopub.status.idle": "2025-01-15T22:44:08.697549Z",
     "shell.execute_reply": "2025-01-15T22:44:08.696728Z",
     "shell.execute_reply.started": "2025-01-15T22:44:08.680308Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sample['pred_len'] = test_sample[\"predictions_ft_clean\"].apply(lambda x: len(x))\n",
    "test_sample['gt_len'] = test_sample[\"hindi\"].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:44:09.835378Z",
     "iopub.status.busy": "2025-01-15T22:44:09.835119Z",
     "iopub.status.idle": "2025-01-15T22:44:10.370736Z",
     "shell.execute_reply": "2025-01-15T22:44:10.369699Z",
     "shell.execute_reply.started": "2025-01-15T22:44:09.835352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score for LoRA fine tuned model: 0.3468\n"
     ]
    }
   ],
   "source": [
    "test_sample[\"BLEU_Score_ft\"] = test_sample[\n",
    "    [\"hindi\", \"predictions_ft_clean\"]\n",
    "].apply(\n",
    "    lambda inputs: calculate_bleu_score(inputs[0], inputs[1]), axis=1\n",
    ")\n",
    "print(f\"Average BLEU Score for LoRA fine tuned model: {test_sample['BLEU_Score_ft'].mean().round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the BLEU Score for our test improved slightly using fine tuned model. Below we see some sample translations that had a high BLEU score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:52:17.487156Z",
     "iopub.status.busy": "2025-01-15T22:52:17.486760Z",
     "iopub.status.idle": "2025-01-15T22:52:17.502258Z",
     "shell.execute_reply": "2025-01-15T22:52:17.501377Z",
     "shell.execute_reply.started": "2025-01-15T22:52:17.487124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "      <th>predictions_ft</th>\n",
       "      <th>predictions_ft_clean</th>\n",
       "      <th>pred_len</th>\n",
       "      <th>gt_len</th>\n",
       "      <th>BLEU_Score_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'en': 'The technology is there to do it.', 'h...</td>\n",
       "      <td>The technology is there to do it.</td>\n",
       "      <td>ऐसा करने के लिए प्रौद्योगिकी है।</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>यह ऐसा करने के लिए प्रौद्योगिकी है।</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>0.809107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>{'en': 'People are joining them due to the fea...</td>\n",
       "      <td>People are joining them due to the fear of Tru...</td>\n",
       "      <td>तृणमूल कांग्रेस के आतंक से डर कर लोग उसमें शाम...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>ही वे लोग इस्लाम के ख़ौदा के डर को अपने द्वारा...</td>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>0.759836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>{'en': 'This time we are warned that an indepe...</td>\n",
       "      <td>This time we are warned that an independent Sc...</td>\n",
       "      <td>इस बार हमें चेतावनी दी गई है कि ई.यू. की सदस्य...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>इसके लिए भारत सरकार ने इस बात की पुष्टि दी कि ...</td>\n",
       "      <td>183</td>\n",
       "      <td>172</td>\n",
       "      <td>0.748740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>{'en': '\"Possibly,\" Martin replied.', 'hi': '\"...</td>\n",
       "      <td>\"Possibly,\" Martin replied.</td>\n",
       "      <td>\"संभवतः,\" मार्टिन ने कहा।</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>\"मैं तुम्हें यह संभव नही\"</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0.731110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>{'en': 'There are many sources from which to c...</td>\n",
       "      <td>There are many sources from which to collect t...</td>\n",
       "      <td>शेष राशि के लिए कई स्रोत है।</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\n Translate the sentence i...</td>\n",
       "      <td>उनकी कई अन्य योग्य स्रोत होते हैं।</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>0.731110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            translation  \\\n",
       "13    {'en': 'The technology is there to do it.', 'h...   \n",
       "2463  {'en': 'People are joining them due to the fea...   \n",
       "907   {'en': 'This time we are warned that an indepe...   \n",
       "772   {'en': '\"Possibly,\" Martin replied.', 'hi': '\"...   \n",
       "1744  {'en': 'There are many sources from which to c...   \n",
       "\n",
       "                                                english  \\\n",
       "13                    The technology is there to do it.   \n",
       "2463  People are joining them due to the fear of Tru...   \n",
       "907   This time we are warned that an independent Sc...   \n",
       "772                         \"Possibly,\" Martin replied.   \n",
       "1744  There are many sources from which to collect t...   \n",
       "\n",
       "                                                  hindi  \\\n",
       "13                     ऐसा करने के लिए प्रौद्योगिकी है।   \n",
       "2463  तृणमूल कांग्रेस के आतंक से डर कर लोग उसमें शाम...   \n",
       "907   इस बार हमें चेतावनी दी गई है कि ई.यू. की सदस्य...   \n",
       "772                           \"संभवतः,\" मार्टिन ने कहा।   \n",
       "1744                       शेष राशि के लिए कई स्रोत है।   \n",
       "\n",
       "                                         predictions_ft  \\\n",
       "13    <start_of_turn>user\\n Translate the sentence i...   \n",
       "2463  <start_of_turn>user\\n Translate the sentence i...   \n",
       "907   <start_of_turn>user\\n Translate the sentence i...   \n",
       "772   <start_of_turn>user\\n Translate the sentence i...   \n",
       "1744  <start_of_turn>user\\n Translate the sentence i...   \n",
       "\n",
       "                                   predictions_ft_clean  pred_len  gt_len  \\\n",
       "13                  यह ऐसा करने के लिए प्रौद्योगिकी है।        35      32   \n",
       "2463  ही वे लोग इस्लाम के ख़ौदा के डर को अपने द्वारा...        66      60   \n",
       "907   इसके लिए भारत सरकार ने इस बात की पुष्टि दी कि ...       183     172   \n",
       "772                           \"मैं तुम्हें यह संभव नही\"        25      25   \n",
       "1744                 उनकी कई अन्य योग्य स्रोत होते हैं।        34      28   \n",
       "\n",
       "      BLEU_Score_ft  \n",
       "13         0.809107  \n",
       "2463       0.759836  \n",
       "907        0.748740  \n",
       "772        0.731110  \n",
       "1744       0.731110  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.sort_values(by = 'BLEU_Score_ft', ascending = False).head(1)\n",
    "sample_trn = test_sample[[\"hindi\", \"predictions_ft_clean\", \"english\", \"BLEU_Score\"]].head(1).to_dict()\n",
    "print(f\"Original english sentence :\\n{sample_trn['english']}\")\n",
    "print(f\"Hindi ground truth sentence : \\n{sample_trn['hindi']}\")\n",
    "print(f\"Hindi translation :\\n{sample_trn['predictions_ft_clean']\")\n",
    "print(f\"BLEU score for this sentence :\\n{sample_trn['BLEU_Score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our fine tuned model still is not able translate the sentences well. We need to use bigger dataset during finetuning to further improve our model's performance. For this exercise we made use of 2000 samples to train the model for 20 epochs which took around 5 hours in Kaggle's notebook (P100 GPU enabled). In future experiments, we plan to use bigger dataset and other memory efficient techniques like quantization to fine tune Gemma model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6332047,
     "sourceId": 10239463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332048,
     "sourceId": 10239465,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332485,
     "sourceId": 10240070,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6334381,
     "sourceId": 10242812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6387023,
     "sourceId": 10316751,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 78150,
     "modelInstanceId": 72246,
     "sourceId": 85986,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
